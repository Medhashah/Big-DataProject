
!!! nohup ssh hz2103@dumbo.hpc.nyu.edu
!!! appending output to nohup.out

business name, play ground, school name (not abbrv)

time spark-submit --master yarn --deploy-mode cluster --verbose --driver-memory 32G --num-executors 4 --executor-cores 5 task1.py 1894 1900

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --conf spark.driver.maxResultSize=2G --driver-memory 16G task1.py 1894 1900

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --conf spark.driver.maxResultSize=2G --driver-memory 16G task1.py 1890 1891

yarn application -list | grep hz2103
application_1569350662793_87900	  mapCombineByKey.py	               SPARK	    hz2103	root.users.hz2103	          ACCEPTED	         UNDEFINED	             0%	                                N/A
application_1569350662793_87895	  mapCombineByKey.py	               SPARK	    hz2103	root.users.hz2103	          ACCEPTED	         UNDEFINED	             0%	                                N/A
application_1569350662793_87290	            task1.py	               SPARK	    hz2103	root.users.hz2103	          ACCEPTED	         UNDEFINED	             0%	                                N/


time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --conf spark.driver.maxResultSize=2G --driver-memory 16G task1.py 1884 1885

1887 1884

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1879 1880

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1883 1884

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1884 1885

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1885 1886

--------------------------------

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1886 1887

-------

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1887 1888

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1888 1889

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1889 1890

how to login to specific node, dumbo hpc!!!!!

module load python/gnu/3.6.5

module load spark/2.4.0

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1885 1886

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1884 1885

sleep 1

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1883 1884

sleep 3600

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1882 1883

a

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1881 1882

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1880 1881

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1879 1880

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1876 1877

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=3G --driver-memory 12G task1.py 1890 1900

time spark-submit --master local[6] --conf spark.executor.memoryOverhead=3G --driver-memory 4G task1.py 1874 1900

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=3G --driver-memory 8G task1.py 1874 1900

time spark-submit --master yarn --deploy-mode cluster --verbose --driver-memory 56G --num-executors 4 --executor-cores 5 task1.py 0 10

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=16G --driver-memory 48G task1.py 1662 1663

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=16G --driver-memory 32G mapReduceVersion.py 1799 1800

time spark-submit --master yarn --deploy-mode cluster --verbose --driver-memory 56G --num-executors 4 --executor-cores 5 mapCombineByKey.py 1872 1900

time spark-submit --driver-memory 16G --num-executors 4 --executor-cores 5 mapCombineByKey.py 1872 1900

time spark-submit --driver-memory 56G --num-executors 4 --executor-cores 5 mapReduceVersion.py 1872 1900

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=16G --driver-memory 32G mapReduceVersion.py 1662 1663

	real	1m53.417s
	user	0m21.427s
	sys	0m5.343s

time spark-submit --driver-memory 48G --num-executors 3 --executor-cores 5 task1.py 1662 1663

	real	1m54.696s
	user	1m36.620s
	sys	0m17.482s	

time spark-submit --driver-memory 48G --num-executors 3 --executor-cores 5 mapReduceVersion.py 1662 1663

	real	1m20.936s
	user	0m32.664s
	sys	0m8.345s

time spark-submit --driver-memory 56G --num-executors 4 --executor-cores 5 mapReduceVersion.py 1662 1663

	real	1m22.922s
	user	0m31.677s
	sys	0m8.122s

time spark-submit --driver-memory 56G --num-executors 7 --executor-cores 5 mapReduceVersion.py 1662 1663

	real	1m22.972s
	user	0m31.422s
	sys	0m9.464s

time spark-submit --driver-memory 56G --num-executors 2 --executor-cores 5 mapReduceVersion.py 1662 1663

	real	1m23.961s
	user	0m32.938s
	sys	0m7.276s

time spark-submit --driver-memory 56G --num-executors 2 --executor-cores 5 mapReduceVersion.py 1799 1800

	real	7m36.807s
	user	0m35.529s
	sys	0m11.015s

time spark-submit --driver-memory 56G --num-executors 4 --executor-cores 5 mapReduceVersion.py 1799 1800

	real	7m25.201s
	user	0m37.158s
	sys	0m7.902s

time spark-submit --driver-memory 56G --num-executors 4 --executor-cores 5 mapAvoidGroupByKey.py 1799 1800

	keyboard interrupt: 13m10.592s

time spark-submit --driver-memory 56G --num-executors 4 --executor-cores 5 mapCombineByKey.py 1799 1800
	
	real	7m3.760s
	user	0m39.668s
	sys	0m8.706s

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=16G --driver-memory 32G mapCombineByKey.py 1662 1663

	

spark-submit --master yarn --deploy-mode cluster --verbose --driver-memory 48G --executor-memory 16G --num-executors 3 --executor-cores 5 task1.py 1662 1663


time spark-submit   --master local[8]   --conf spark.executor.memoryOverhead=3G   --executor-memory 6G   task1.py

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=16G --driver-memory 32G --executor-memory 32G task1.py 1662 1663

time spark-submit --master local[8] --conf spark.executor.memoryOverhead=16G --driver-memory 32G task1.py 1662 1663

time spark-submit \
  --num-executors 4 \
  --executor-cores 2 \
  --conf spark.executor.memoryOverhead=16G \
  --driver-memory 32G \
  --executor-memory 32G \
  task1.py 1662 1663

time spark-submit   --master local[8]   --conf spark.executor.memoryOverhead=3G   --executor-memory 6G   speedupTest.py 1662 1663

time spark-submit task1.py


628 454 after 10min: 649 473; after another 10 min 663, 496, 808 (tot 566)

Questions:

1. do we need to append task2 json into task1 json?

Task1 Note: 

	they all stopped at col names with '.'!!!
		'`Family Size (3 nos.) Monthly Income`'
		'`St. Nicholas`'
		'`TOTAL NO. OF PROPERTIES`'

	duplicated column names (difference only in white spaces ==> poor naming) [Data Quality Issues]

	0-200 
		stopped at 52 12/03/01:37 AM skip 52 datasetName[51]
			cannot resolve '`Family Size (3 nos.) Monthly Income`' given input columns: [Family Size (5 nos.) Monthly Income, Each Additional Person Add, Family Size (1 ind.) Monthly Income, Family Size (no.), Family Size (2 nos.) Monthly Income, Family Size (3 nos.) Monthly Income, Family Size (4 nos.) Monthly Income];;\n'Project ['Family Size (3 nos.) Monthly Income]\n+- Project [Family Size (no.)#10 AS Family Size (no.)#24, Family Size (1 ind.) Monthly Income#11 AS Family Size (1 ind.) Monthly Income#25, Family Size (2 nos.) Monthly Income#12 AS Family Size (2 nos.) Monthly Income#26, Family Size (3 nos.) Monthly Income#13 AS Family Size (3 nos.) Monthly Income#27, Family Size (4 nos.) Monthly Income#14 AS Family Size (4 nos.) Monthly Income#28, Family Size (5 nos.) Monthly Income#15 AS Family Size (5 nos.) Monthly Income#29, Each Additional Person Add#16 AS Each Additional Person Add#30]\n   +- Relation[Family Size (no.)#10,Family Size (1 ind.) Monthly Income#11,Family Size (2 nos.) Monthly Income#12,Family Size (3 nos.) Monthly Income#13,Family Size (4 nos.) Monthly Income#14,Family Size (5 nos.) Monthly Income#15,Each Additional Person Add#16] csv\n"
		stopped at 93
	200-400 
		stopped at 252 12/03/01:57 AM skip 252 datasetName[251]
			pyspark.sql.utils.AnalysisException: "cannot resolve '`St. Nicholas`' given input columns: [Citywide Monthly Average Wait Time, Jamaica, Coney Island, Hunt's Point, Fort Greene, Richmond, Waverly, Rockaway, Washington Heights, Queens, East New York, East End, Crotona, Total Monthly Average NCA SNAP, Concourse, SSI, Center Name, Williamsburg, North Brooklyn, St. Nicholas];;\n'Project ['St. Nicholas]\n+- Project [Center Name#10 AS Center Name#50, Concourse#11 AS Concourse#51, Coney Island#12 AS Coney Island#52, Crotona#13 AS Crotona#53, North Brooklyn#14 AS North Brooklyn#54, Washington Heights#15 AS Washington Heights#55, East End#16 AS East End#56, East New York#17 AS East New York#57, Fort Greene#18 AS Fort Greene#58, Hunt's Point#19 AS Hunt's Point#59, Jamaica#20 AS Jamaica#60, Queens#21 AS Queens#61, Richmond#22 AS Richmond#62, Rockaway#23 AS Rockaway#63, SSI#24 AS SSI#64, St. Nicholas#25 AS St. Nicholas#65, Waverly#26 AS Waverly#66, Williamsburg#27 AS Williamsburg#67, Total Monthly Average NCA SNAP#28 AS Total Monthly Average NCA SNAP#68, Citywide Monthly Average Wait Time#29 AS Citywide Monthly Average Wait Time#69]\n   +- Relation[Center Name#10,Concourse#11,Coney Island#12,Crotona#13,North Brooklyn#14,Washington Heights#15,East End#16,East New York#17,Fort Greene#18,Hunt's Point#19,Jamaica#20,Queens#21,Richmond#22,Rockaway#23,SSI#24,St. Nicholas#25,Waverly#26,Williamsburg#27,Total Monthly Average NCA SNAP#28,Citywide Monthly Average Wait Time#29] csv\n"
		stopped at 262,skipped 12/03/02:08 cannot resolve '`TOTAL NO. OF PROPERTIES`' given input columns: [NEIGHBORHOOD, ME ==> same problem maybe of ` `
	420:
		========================================
		Processing file: 56bx-u7iw (#420 of 1900)
		----------------------------------------
		Processing column: b'Month'
		----------------------------------------
		Processing column: b'Cash Assistance Recipients'
		----------------------------------------
		Processing column: b'FAP (formerly AFDC)'
		----------------------------------------
		Processing column: b'60 Month converted to SNA'
		Traceback (most recent call last):
		  File "/share/apps/spark/spark-2.4.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
		  File "/share/apps/spark/spark-2.4.0-bin-hadoop2.6/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
		py4j.protocol.Py4JJavaError: An error occurred while calling o13232.select.
		: org.apache.spark.sql.AnalysisException: Reference '60 Month converted to SNA' is ambiguous, could be: 60 Month converted to SNA, 60 Month converted to SNA, 60 Month converted to SNA.;

Count number of json files
ls -l json/ | wc -l

Todo:

1. debug cannot resolve '`TOTAL NO. OF PROPERTIES`' 
2. parallel