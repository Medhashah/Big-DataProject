{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext, RDD\n",
    "from csv import reader\n",
    "import itertools\n",
    "import rdd_util\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext, RDD\n",
    "from csv import reader\n",
    "import itertools\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "def is_date(s: str, col: str):\n",
    "    col_name = col.lower()\n",
    "    if 'date' in col_name or 'year' in col_name \\\n",
    "            or 'time' in col_name or 'month' in col_name \\\n",
    "            or 'day' in col_name:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def mapd(x: List):\n",
    "    \"\"\"\n",
    "    TODO: check date type\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # [col_idx, (value, type)]\n",
    "    res = (x[0], [x[1], None])\n",
    "    if (x[1] == ''):\n",
    "        res[1][1] = 'empty'\n",
    "    elif(is_date(x[1],x[0])):\n",
    "        res[1][1] = 'date'\n",
    "    elif (is_int(x[1])):\n",
    "        res[1][1] = 'int'\n",
    "        res[1][0] = int(res[1][0])\n",
    "    elif (is_float(x[1])):\n",
    "        res[1][1] = 'real'\n",
    "        res[1][0] = float(res[1][0])\n",
    "    else:\n",
    "        res[1][1] = 'text'\n",
    "    return res\n",
    "\n",
    "\n",
    "def is_int(s: str):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_float(value: str):\n",
    "    if '.' not in value:\n",
    "        return False\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_meta(spark: SparkSession, path: str):\n",
    "    # read dataframe\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    # Add index to each row, [([...], 0),([...], 1)...]\n",
    "    rdd = sc.textFile(path, 1).mapPartitions(lambda x: reader(x, delimiter='\\t')).zipWithIndex()\n",
    "    header = rdd.filter(lambda x: x[1] == 0) \\\n",
    "        .map(lambda x: (x[0])).collect()[0]  # extract the first part, ignore idx\n",
    "    rows = rdd.filter(lambda x: x[1] != 0).map(lambda x: x[0])\n",
    "    file_name = path.split('/')[-1]\n",
    "    metadata = {\n",
    "        'dataset_name': file_name,\n",
    "        'key_column_candidates': header\n",
    "    }\n",
    "    N = len(header)\n",
    "    # Transform to [(col_idx, value),(col_idx, value)...]\n",
    "    items = rows.flatMap(\n",
    "        lambda x, h=header: [(h[i], x[i]) for i in range(N)])\n",
    "\n",
    "    # Transform to [(col_idx, (value, type)),(col_idx, (value, type))...]\n",
    "    mapped_items = items.map(mapd)\n",
    "    col_map = {}\n",
    "    for col in header:\n",
    "        col_map[col] = {}\n",
    "\n",
    "    res2 = generate_distinct_top5(items)\n",
    "    res1 = generate_null_empty(mapped_items)\n",
    "    # [(col,non-empty, empty, total, distinct_num, top5:(col_name,freq))]\n",
    "    flat_res = res1.join(res2).map(lambda x: (x[0], (*x[1][0], *x[1][1]))).collect()\n",
    "    columns = []\n",
    "    for res in flat_res:\n",
    "        column_data = {\n",
    "            'column_name': res[0],\n",
    "            'number_non_empty_cells': res[1][0],\n",
    "            'number_empty_cells': res[1][1],\n",
    "            'number_distinct_values': res[1][3],\n",
    "            'frequent_values': [x[0] for x in res[1][4]]\n",
    "        }\n",
    "        columns.append(column_data)\n",
    "    metadata['columns'] =columns\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def generate_null_empty(mapped_items: RDD) -> RDD:\n",
    "    \"\"\"\n",
    "    :param mapped_items: [(col,(value, type)), ...]\n",
    "    :return: [(col1,[non-empty, empty, total]), (col2,[null-empty, empty, total])]\n",
    "    \"\"\"\n",
    "\n",
    "    def seqFunc(local, x):\n",
    "        res = [i for i in local];\n",
    "        if (x[1] != 'empty'):\n",
    "            res[0] = local[0] + 1\n",
    "        else:\n",
    "            res[1] = local[1] + 1\n",
    "        res[2] = local[2] + 1\n",
    "        return res\n",
    "\n",
    "    combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "    count = mapped_items.aggregateByKey((0, 0, 0), seqFunc, combFunc)\n",
    "    return count\n",
    "\n",
    "\n",
    "def generate_distinct_top5(items: RDD) -> RDD:\n",
    "    \"\"\"\n",
    "    :param items: [(col,value),...]\n",
    "    :return: [(col,(distinct_num, [top5...])),(col,(distinct_num, [top5...])),...]\n",
    "    \"\"\"\n",
    "    freq_items = items.map(lambda x: ((x[0], x[1]), 1)) \\\n",
    "        .aggregateByKey((0, 0),\n",
    "                        (lambda x, y: (0, x[1] + 1)),\n",
    "                        (lambda x, y: (x[1] + y[1]))) \\\n",
    "        .map(lambda x: ((x[0][0]), (x[0][1], x[1][1])))\n",
    "    sorted_grouped_freq_items = freq_items.sortBy(lambda x: x[1][1], ascending=False).groupByKey()\n",
    "    res = sorted_grouped_freq_items.mapValues(lambda x: (len(x), list(itertools.islice(x, 5))))\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/user/hm74/NYCOpenData/2bmr-jdsv.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 ms, sys: 5 ms, total: 23 ms\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read dataframe\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Add index to each row, [([...], 0),([...], 1)...]\n",
    "rdd = sc.textFile(path, 10).mapPartitions(lambda x: reader(x, delimiter='\\t')).zipWithIndex()\n",
    "header = rdd.filter(lambda x: x[1] == 0) \\\n",
    "    .map(lambda x: (x[0])).collect()[0]  # extract the first part, ignore idx\n",
    "rows = rdd.filter(lambda x: x[1] != 0).map(lambda x: x[0])\n",
    "file_name = path.split('/')[-1]\n",
    "metadata = {\n",
    "    'dataset_name': file_name,\n",
    "    'key_column_candidates': header\n",
    "}\n",
    "N = len(header)\n",
    "# Transform to [(col_idx, value),(col_idx, value)...]\n",
    "items = rows.flatMap(\n",
    "    lambda x, h=header: [(h[i], x[i]) for i in range(N)])\n",
    "\n",
    "# Transform to [(col_idx, (value, type)),(col_idx, (value, type))...]\n",
    "mapped_items = items.map(mapd).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Base_Number', ['B02756', 'text']),\n",
       " ('Wave_Number', [3, 'int']),\n",
       " ('Base_Name', ['ALLY CAR SERVICE LLC', 'text']),\n",
       " ('DBA', ['ACTIVE EXPRESS CAR & LIMO 2', 'text']),\n",
       " ('years', ['2015', 'date']),\n",
       " ('Week_Number', [40, 'int']),\n",
       " ('Pickup_Start_Date', ['09/27/2015 12:00:00 AM', 'date']),\n",
       " ('Pickup_End_Date', ['10/03/2015 12:00:00 AM', 'date']),\n",
       " ('Total_Dispatched_Trips', [19, 'int']),\n",
       " ('Unique_Dispatched_Vehicle', [6, 'int'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_items.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearrange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_type_items = mapped_items.map(lambda x: ((x[0],x[1][1]),x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Base_Number', 'text'), 'B02756'),\n",
       " (('Wave_Number', 'int'), 3),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('years', 'date'), '2015'),\n",
       " (('Week_Number', 'int'), 40),\n",
       " (('Pickup_Start_Date', 'date'), '09/27/2015 12:00:00 AM'),\n",
       " (('Pickup_End_Date', 'date'), '10/03/2015 12:00:00 AM'),\n",
       " (('Total_Dispatched_Trips', 'int'), 19),\n",
       " (('Unique_Dispatched_Vehicle', 'int'), 6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_type_items.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Base_Number', 'text'), 'B02756'),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('Base_Number', 'text'), 'B02756'),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('Base_Number', 'text'), 'B02756'),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('Base_Number', 'text'), 'B02756')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_text_type_items = col_type_items.filter(lambda x: x[0][1]=='text')\n",
    "col_text_type_items.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shortest and longest, average length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Base_Number', 'text'), 'B02756'),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('Base_Number', 'text'), 'B02756'),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('Base_Number', 'text'), 'B02756'),\n",
       " (('Base_Name', 'text'), 'ALLY CAR SERVICE LLC'),\n",
       " (('DBA', 'text'), 'ACTIVE EXPRESS CAR & LIMO 2'),\n",
       " (('Base_Number', 'text'), 'B02756')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_text_type_items.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqFunc(local, x):\n",
    "#     Not includes empty text\n",
    "    if local[0] == '#':\n",
    "        shortest = x\n",
    "    else:\n",
    "        shortest = x if len(x)<len(local[0]) else local[0]        \n",
    "    longest = x if len(x)>len(local[1]) else local[1]\n",
    "    total_len = local[2] + len(x)\n",
    "    count =local[3] + 1\n",
    "    return (shortest, longest, total_len, count)\n",
    "\n",
    "combFunc = (lambda x, y: (x[0] if len(x[0])<len(y[0]) else y[0], x[1] if len(x[1])>len(y[1]) else y[1],\n",
    "                         x[2]+y[2], x[3]+y[3]))\n",
    "shortest_and_longest = col_text_type_items.aggregateByKey(('#','',0,0), seqFunc, combFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Base_Number', 'text'), ('B02756', 'B02756', 606198, 101033)),\n",
       " (('Base_Name', 'text'),\n",
       "  ('ALEX II',\n",
       "   'ROMERO AUTO SERVICE REPAIRS TRANSPORT TOWING RECOVERY LLC',\n",
       "   2385987,\n",
       "   101033)),\n",
       " (('DBA', 'text'),\n",
       "  ('GTS', 'TRISTAR WORLDWIDE CHAUFFEURED SERVICES', 454524, 24422))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_and_longest.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic=shortest_and_longest.map(lambda x: (x[0],(x[1][0],x[1][1],x[1][2]/x[1][3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Base_Number', 'text'), ('B02756', 'B02756', 6.0)),\n",
       " (('Base_Name', 'text'),\n",
       "  ('ALEX II',\n",
       "   'ROMERO AUTO SERVICE REPAIRS TRANSPORT TOWING RECOVERY LLC',\n",
       "   23.615917571486545)),\n",
       " (('DBA', 'text'),\n",
       "  ('GTS', 'TRISTAR WORLDWIDE CHAUFFEURED SERVICES', 18.611252149701087))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_statistic(col_num_type_items: RDD) -> RDD:\n",
    "    \"\"\"\n",
    "    :param col_num_type_items: [(('Wave_Number', 'int'), 3),(('Week_Number', 'int'), 40)...]\n",
    "    :return: ['Wave_Number', 'int'], [max_value, min_value, sum, count, mean, std])\n",
    "    \"\"\"\n",
    "\n",
    "    def seqFunc(local, x):\n",
    "        max_value = x if x > local[0] else local[0]\n",
    "        min_value = x if x < local[1] else local[1]\n",
    "        return (max_value, min_value, local[2] + x, local[3] + 1)\n",
    "\n",
    "    combFunc = (lambda x, y: (max(x[0], y[0]), min(x[1], y[1]), x[2] + y[2], x[3] + y[3]))\n",
    "    num_statistic = col_num_type_items.aggregateByKey((0, 0, 0, 0), seqFunc, combFunc)\n",
    "    num_statistic = num_statistic.map(lambda x: (x[0], [*x[1], x[1][2] / x[1][3]]))\n",
    "    # [(('col_name', 'num_type'),(value, mean))...]\n",
    "    col_num_mean_items = col_num_type_items.join(num_statistic.map(lambda x: (x[0], x[1][4])))\n",
    "    result_dev = col_num_mean_items.aggregateByKey((0,), lambda local, x: (local[0] + (x[0] - x[1]) ** 2,), (lambda x, y: (x[0] + y[0])))\n",
    "    result_std = result_dev.map(lambda x: (x[0], math.sqrt(x[1][0])))\n",
    "    return num_statistic.join(result_std).map(lambda x: [x[0],[*x[1][0],x[1][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 ms, sys: 4 ms, total: 48 ms\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res= generate_num_statistic(col_num_type_items)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Wave_Number', 'int'),\n",
       "  [4, 0, 272219, 101033, 2.6943572892025376, 278.26561607671164]],\n",
       " [('Unique_Dispatched_Vehicle', 'int'),\n",
       "  [33578, 0, 16209232, 101033, 160.43502617956509, 337302.72844989697]],\n",
       " [('Week_Number', 'int'),\n",
       "  [53, 0, 2628258, 101033, 26.013856858650144, 4727.1895033303]],\n",
       " [('Total_Dispatched_Trips', 'int'),\n",
       "  [909056, 0, 549722961, 101033, 5441.0238337968785, 9663047.709473047]]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-2.3.0 / PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
