{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext, RDD\n",
    "from csv import reader\n",
    "import itertools\n",
    "import rdd_util\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"project\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext, RDD\n",
    "from csv import reader\n",
    "import itertools\n",
    "\n",
    "\n",
    "def mapd(x: List):\n",
    "    \"\"\"\n",
    "    TODO: check date type\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # [col_idx, (value, type)]\n",
    "    res = (x[0], [x[1], None])\n",
    "    if (x[1] == ''):\n",
    "        res[1][1] = 'empty'\n",
    "    elif (is_int(x[1])):\n",
    "        res[1][1] = 'int'\n",
    "    elif (is_float(x[1])):\n",
    "        res[1][1] = 'real'\n",
    "    else:\n",
    "        res[1][1] = 'text'\n",
    "    return res\n",
    "\n",
    "\n",
    "def is_int(s: str):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_float(value: str):\n",
    "    if '.' not in value:\n",
    "        return False\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_meta(spark: SparkSession, path: str):\n",
    "    # read dataframe\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    # Add index to each row, [([...], 0),([...], 1)...]\n",
    "    rdd = sc.textFile(path, 1).mapPartitions(lambda x: reader(x, delimiter='\\t')).zipWithIndex()\n",
    "    header = rdd.filter(lambda x: x[1] == 0) \\\n",
    "        .map(lambda x: (x[0])).collect()[0]  # extract the first part, ignore idx\n",
    "    rows = rdd.filter(lambda x: x[1] != 0).map(lambda x: x[0])\n",
    "    file_name = path.split('/')[-1]\n",
    "    metadata = {\n",
    "        'dataset_name': file_name,\n",
    "        'key_column_candidates': header\n",
    "    }\n",
    "    N = len(header)\n",
    "    # Transform to [(col_idx, value),(col_idx, value)...]\n",
    "    items = rows.flatMap(\n",
    "        lambda x, h=header: [(h[i], x[i]) for i in range(N)])\n",
    "\n",
    "    # Transform to [(col_idx, (value, type)),(col_idx, (value, type))...]\n",
    "    mapped_items = items.map(mapd)\n",
    "    col_map = {}\n",
    "    for col in header:\n",
    "        col_map[col] = {}\n",
    "\n",
    "    res2 = generate_distinct_top5(items)\n",
    "    res1 = generate_null_empty(mapped_items)\n",
    "    # [(col,non-empty, empty, total, distinct_num, top5:(col_name,freq))]\n",
    "    flat_res = res1.join(res2).map(lambda x: (x[0], (*x[1][0], *x[1][1]))).collect()\n",
    "    columns = []\n",
    "    for res in flat_res:\n",
    "        column_data = {\n",
    "            'column_name': res[0],\n",
    "            'number_non_empty_cells': res[1][0],\n",
    "            'number_empty_cells': res[1][1],\n",
    "            'number_distinct_values': res[1][3],\n",
    "            'frequent_values': [x[0] for x in res[1][4]]\n",
    "        }\n",
    "        columns.append(column_data)\n",
    "    metadata['columns'] =columns\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def generate_null_empty(mapped_items: RDD) -> RDD:\n",
    "    \"\"\"\n",
    "    :param mapped_items: [(col,(value, type)), ...]\n",
    "    :return: [(col1,[non-empty, empty, total]), (col2,[null-empty, empty, total])]\n",
    "    \"\"\"\n",
    "\n",
    "    def seqFunc(local, x):\n",
    "        res = [i for i in local];\n",
    "        if (x[1] != 'empty'):\n",
    "            res[0] = local[0] + 1\n",
    "        else:\n",
    "            res[1] = local[1] + 1\n",
    "        res[2] = local[2] + 1\n",
    "        return res\n",
    "\n",
    "    combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "    count = mapped_items.aggregateByKey((0, 0, 0), seqFunc, combFunc)\n",
    "    return count\n",
    "\n",
    "\n",
    "def generate_distinct_top5(items: RDD) -> RDD:\n",
    "    \"\"\"\n",
    "    :param items: [(col,value),...]\n",
    "    :return: [(col,(distinct_num, [top5...])),(col,(distinct_num, [top5...])),...]\n",
    "    \"\"\"\n",
    "    freq_items = items.map(lambda x: ((x[0], x[1]), 1)) \\\n",
    "        .aggregateByKey((0, 0),\n",
    "                        (lambda x, y: (0, x[1] + 1)),\n",
    "                        (lambda x, y: (x[1] + y[1]))) \\\n",
    "        .map(lambda x: ((x[0][0]), (x[0][1], x[1][1])))\n",
    "    sorted_grouped_freq_items = freq_items.sortBy(lambda x: x[1][1], ascending=False).groupByKey()\n",
    "    res = sorted_grouped_freq_items.mapValues(lambda x: (len(x), list(itertools.islice(x, 5))))\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/user/hm74/NYCOpenData/2232-dj5q.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.53 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_name': '2232-dj5q.tsv.gz',\n",
       " 'key_column_candidates': ['category',\n",
       "  'single men',\n",
       "  'single women',\n",
       "  'total single adults',\n",
       "  'families with children',\n",
       "  'total families',\n",
       "  'total adults in families',\n",
       "  'total children',\n",
       "  'data period'],\n",
       " 'columns': [{'column_name': 'category',\n",
       "   'number_non_empty_cells': 176,\n",
       "   'number_empty_cells': 9,\n",
       "   'number_distinct_values': 18,\n",
       "   'frequent_values': ['number of individuals who are on wait-list - DYCD-administered transitional independent living facilities',\n",
       "    'Average Length of Stay: DHS -administered facility (by type, excluding drop-in and faith-based)',\n",
       "    'Average Length of Stay: DYCD -administered crisis facility',\n",
       "    'number of unduplicated persons - DYCD-administered facilities',\n",
       "    'number of unduplicated persons - DYCD-administered crisis shelters']},\n",
       "  {'column_name': 'single men',\n",
       "   'number_non_empty_cells': 89,\n",
       "   'number_empty_cells': 96,\n",
       "   'number_distinct_values': 71,\n",
       "   'frequent_values': ['', '0.92', '0.91', '0.93', '0.89']},\n",
       "  {'column_name': 'single women',\n",
       "   'number_non_empty_cells': 44,\n",
       "   'number_empty_cells': 141,\n",
       "   'number_distinct_values': 41,\n",
       "   'frequent_values': ['', '60', '27', '38', '61']},\n",
       "  {'column_name': 'total single adults',\n",
       "   'number_non_empty_cells': 44,\n",
       "   'number_empty_cells': 141,\n",
       "   'number_distinct_values': 39,\n",
       "   'frequent_values': ['', '117', '77', '42', '113']},\n",
       "  {'column_name': 'families with children',\n",
       "   'number_non_empty_cells': 33,\n",
       "   'number_empty_cells': 152,\n",
       "   'number_distinct_values': 18,\n",
       "   'frequent_values': ['', '7', '5', '8', '13']},\n",
       "  {'column_name': 'total families',\n",
       "   'number_non_empty_cells': 33,\n",
       "   'number_empty_cells': 152,\n",
       "   'number_distinct_values': 18,\n",
       "   'frequent_values': ['', '7', '5', '8', '13']},\n",
       "  {'column_name': 'total adults in families',\n",
       "   'number_non_empty_cells': 34,\n",
       "   'number_empty_cells': 151,\n",
       "   'number_distinct_values': 20,\n",
       "   'frequent_values': ['', '7', '8', '4', '3']},\n",
       "  {'column_name': 'total children',\n",
       "   'number_non_empty_cells': 54,\n",
       "   'number_empty_cells': 131,\n",
       "   'number_distinct_values': 38,\n",
       "   'frequent_values': ['', '3', '0.95', '10', '5']},\n",
       "  {'column_name': 'data period',\n",
       "   'number_non_empty_cells': 185,\n",
       "   'number_empty_cells': 0,\n",
       "   'number_distinct_values': 11,\n",
       "   'frequent_values': ['201908', '201905', '201906', '201907', '201810']}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "res = generate_meta(spark,path)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(path,1).mapPartitions(lambda x: reader(x, delimiter='\\t')).zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-2.3.0 / PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
